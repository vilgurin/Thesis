{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-28T14:59:29.732501Z","iopub.status.busy":"2024-04-28T14:59:29.731897Z","iopub.status.idle":"2024-04-28T14:59:30.540029Z","shell.execute_reply":"2024-04-28T14:59:30.539243Z","shell.execute_reply.started":"2024-04-28T14:59:29.732471Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:30.541892Z","iopub.status.busy":"2024-04-28T14:59:30.541504Z","iopub.status.idle":"2024-04-28T14:59:37.707256Z","shell.execute_reply":"2024-04-28T14:59:37.706245Z","shell.execute_reply.started":"2024-04-28T14:59:30.541866Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","from tqdm import tqdm\n","import torch\n","import numpy as np\n","import cv2\n","from PIL import Image\n","from torch import nn\n","from torchvision.models import vgg19\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.feature_extraction import create_feature_extractor\n","from torch.utils.data.dataset import random_split\n","import pickle\n","from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n","from torchvision.transforms import ToPILImage\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms.functional import to_pil_image\n","from scipy.spatial.distance import cosine\n","from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n","from torch import nn"]},{"cell_type":"markdown","metadata":{},"source":["### MODELS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:37.709109Z","iopub.status.busy":"2024-04-28T14:59:37.708634Z","iopub.status.idle":"2024-04-28T14:59:37.738831Z","shell.execute_reply":"2024-04-28T14:59:37.737897Z","shell.execute_reply.started":"2024-04-28T14:59:37.709082Z"},"trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):\n","        \"\"\"\n","        input_nc (int): Number of channels in the input images.\n","        ndf (int): Number of filters in the first convolutional layer.\n","        n_layers (int): Number of convolutional layers.\n","        norm_layer: Normalization layer class.\n","        \"\"\"\n","        super(Discriminator, self).__init__()\n","        kw = 4\n","        padw = 1\n","        stride = 2\n","\n","        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=stride, padding=padw), nn.LeakyReLU(0.2, True)]\n","\n","        nf_mult = 1\n","        nf_mult_prev = 1\n","        for n in range(1, n_layers):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2**n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=stride, padding=padw, bias=False),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2**n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=False),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        self.model = nn.Sequential(*sequence)\n","\n","        self.flatten_size = self._get_flatten_size(input_nc, ndf, n_layers, kw, padw, stride)\n","\n","        self.fc = nn.Linear(self.flatten_size, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def _get_flatten_size(self, input_nc, ndf, n_layers, kw, padw, stride):\n","        with torch.no_grad():\n","            dummy_input = torch.zeros(1, input_nc, 256, 256)\n","            output = self.model(dummy_input)\n","            return int(torch.prod(torch.tensor(output.shape[1:])))\n","\n","    def forward(self, input):\n","        output = self.model(input)\n","        output = output.view(output.size(0), -1)  \n","        output = self.fc(output)  \n","        output = self.sigmoid(output)\n","        return output\n","    \n","    \n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=2, dilation=2),\n","            nn.BatchNorm2d(in_features),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(0.25), \n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=2, dilation=2),\n","            nn.BatchNorm2d(in_features)\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","    \n","    \n","class Generator(nn.Module):\n","    def __init__(self, input_channels, num_residual_blocks=8):\n","        super(Generator, self).__init__()\n","        \n","        self.init_conv = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        \n","        self.down1 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n","                                   nn.BatchNorm2d(128),\n","                                   nn.LeakyReLU(0.2, inplace=True))\n","        self.down2 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n","                                   nn.BatchNorm2d(256),\n","                                   nn.LeakyReLU(0.2, inplace=True))\n","        \n","        self.res_blocks = nn.Sequential(\n","            *[ResidualBlock(256) for _ in range(num_residual_blocks)]\n","        )\n","        \n","        self.up1 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n","                                 nn.BatchNorm2d(128),\n","                                 nn.LeakyReLU(0.2, inplace=True))\n","        self.up2 = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n","                                 nn.BatchNorm2d(64),\n","                                 nn.LeakyReLU(0.2, inplace=True))\n","        \n","        self.out = nn.Sequential(\n","            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n","            nn.Sigmoid()  \n","        )\n","\n","    def forward(self, x):\n","        x_init = self.init_conv(x)\n","        \n","        x_down1 = self.down1(x_init)\n","        x_down2 = self.down2(x_down1)\n","        x_res = self.res_blocks(x_down2)\n","        \n","        x_up1 = self.up1(x_res) + x_down1  \n","        x_up2 = self.up2(x_up1) + x_init \n","        \n","        x_out = self.out(x_up2)\n","        return x_out\n","    \n","class EdgeRefineModule(nn.Module):\n","    def __init__(self, input_channels=5, num_layers=3):\n","        super(EdgeRefineModule, self).__init__()\n","\n","        layers = [\n","            nn.Conv2d(input_channels, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        ]\n","\n","        for _ in range(1, num_layers):  \n","            layers.extend([\n","                nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True)\n","            ])\n","\n","        self.model = nn.Sequential(*layers)\n","        self.final_conv = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)  \n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = self.final_conv(x)\n","        return self.sigmoid(x)"]},{"cell_type":"markdown","metadata":{},"source":["### KINS DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:37.741224Z","iopub.status.busy":"2024-04-28T14:59:37.740931Z","iopub.status.idle":"2024-04-28T14:59:37.752640Z","shell.execute_reply":"2024-04-28T14:59:37.751745Z","shell.execute_reply.started":"2024-04-28T14:59:37.741199Z"},"trusted":true},"outputs":[],"source":["class KINSDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.images_dir = os.path.join(root_dir, 'images')\n","        self.gt_masks_dir = os.path.join(root_dir, 'gt_masks')\n","        self.occ_masks_dir = os.path.join(root_dir, 'occ_masks')\n","        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.images_dir, self.image_files[idx])\n","        gt_mask_name = os.path.join(self.gt_masks_dir, f'gt_mask_{self.image_files[idx].split(\"_\")[-1]}')\n","        occ_mask_name = os.path.join(self.occ_masks_dir, f'occ_mask_{self.image_files[idx].split(\"_\")[-1]}')\n","\n","        image = Image.open(img_name).convert('RGB')\n","        gt_mask = Image.open(gt_mask_name).convert('L') \n","        occ_mask = Image.open(occ_mask_name).convert('L')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","            gt_mask = self.transform(gt_mask)\n","            occ_mask = self.transform(occ_mask)\n","\n","        return {'image': image, 'processed_img_gt': gt_mask, 'processed_img_occ': occ_mask, \"path\": gt_mask_name}\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["### PASCAL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:37.754627Z","iopub.status.busy":"2024-04-28T14:59:37.754274Z","iopub.status.idle":"2024-04-28T14:59:44.285564Z","shell.execute_reply":"2024-04-28T14:59:44.284581Z","shell.execute_reply.started":"2024-04-28T14:59:37.754578Z"},"trusted":true},"outputs":[],"source":["def process_occluder(ann, sz):\n","    occluder = ann[\"occluder_mask\"]\n","    occ_mask = np.zeros(sz)\n","    if occluder is not None:\n","        if occluder.ndim != 0:\n","            for occ in occluder:\n","                occ = np.array(occ)\n","                occ = occ.reshape((-1,1,2))\n","                occ_temp = points2mask(occ, sz)\n","                occ_mask = np.maximum(occ_mask, occ_temp)\n","            return occ_mask\n","    return None\n","\n","def get_bbox(ann, img_size, enlarge_factor = 1.2):\n","    bbox = np.array(ann[\"box\"][0:4]).astype(int)\n","    h, w = img_size\n","    y0, x0, y1, x1 = bbox\n","    width = x1 - x0\n","    height = y1 - y0\n","    delta_width = int((width * enlarge_factor - width) / 2)\n","    delta_height = int((height * enlarge_factor - height) / 2)\n","    new_y0 = max(0, y0 - delta_height)\n","    new_x0 = max(0, x0 - delta_width)\n","    new_y1 = min(h, y1 + delta_height)\n","    new_x1 = min(w, x1 + delta_width)\n","\n","    return [new_y0, new_x0, new_y1, new_x1]\n","\n","def points2mask(points, img_size):\n","    mask = np.zeros(img_size, dtype=np.uint8)\n","    points = points.astype(np.int32)\n","    cv2.fillPoly(mask, [points], 255)\n","    return mask\n","\n","def draw_points(img, points):\n","    for point in points:\n","        cv2.circle(img, tuple(int(x) for x in point), 3, (255, 0, 0), -1)\n","    return img\n","\n","def process_annotation(ann, img_size):\n","    flat_list = [item for sublist in ann[\"mask\"] for item in sublist]\n","    obj_points = np.array(flat_list).reshape((-1, 2)).astype(int)\n","    try:\n","        obj_mask = points2mask(obj_points, img_size)\n","    except Exception as e:\n","        print(e)\n","        print(obj_points)\n","    \n","    amodal_bbox = get_bbox(ann, img_size)\n","    occluder_mask = process_occluder(ann, img_size)\n","    \n","    if occluder_mask is not None:\n","        final_mask = obj_mask * (255 - occluder_mask)\n","    else:\n","        final_mask = obj_mask \n","    \n","    return final_mask, amodal_bbox\n","\n","def divide_gt(gt_images_path):\n","    names = []\n","    for p in os.listdir(gt_images_path):\n","        names.append(p)\n","        \n","    train, val = train_test_split(names, test_size = 0.2, random_state = 42)\n","    return train, val\n","\n","def process_directories(path):\n","    images = os.path.join(path, \"images\")\n","    gt_images = os.path.join(images, \"carFGL0_BGL0\")\n","    train_img_dirs = [os.path.join(images, i) for i in os.listdir(images) if os.path.join(images, i) != gt_images]\n","\n","    all_train_images = []\n","    all_train_anns = []\n","    all_val_images = []\n","    \n","    _, gt_names_val = divide_gt(gt_images)\n","    \n","    for directory in train_img_dirs:\n","        for file in os.listdir(directory):\n","            if file in gt_names_val:\n","                all_val_images.append(os.path.join(directory, file))\n","            else:\n","                all_train_images.append(os.path.join(directory, file))\n","    \n","    return all_train_images, all_val_images\n","\n","\n","path = \"/kaggle/input/occ-vehicles-final/occluded_vehicles/testing\"\n","all_train_images, all_val_images = process_directories(path)\n","\n","def get_ann_path(image_path):\n","    ann_path = image_path.replace(\"images\", \"anns\").replace(\".JPEG\", \".npz\")\n","    gt_ann_path = ann_path.replace(image_path.split('/')[-2], \"carFGL0_BGL0\")\n","    \n","    return ann_path, gt_ann_path\n","ann_path, gt_ann_path = get_ann_path(\"/kaggle/input/occ-vehicles-final/occluded_vehicles/testing/images/carFGL3_BGL1/n04166281_18171.JPEG\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.287192Z","iopub.status.busy":"2024-04-28T14:59:44.286929Z","iopub.status.idle":"2024-04-28T14:59:44.303808Z","shell.execute_reply":"2024-04-28T14:59:44.302823Z","shell.execute_reply.started":"2024-04-28T14:59:44.287170Z"},"trusted":true},"outputs":[],"source":["class OccludedVehiclesDataset(Dataset):\n","    def __init__(self, image_paths, transform=None):\n","        self.image_paths = image_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","\n","        img_path = self.image_paths[idx]\n","        ann_path_occ, ann_path_gt = get_ann_path(img_path)\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        img_size = image.size[::-1] \n","\n","        ann_occ = np.load(ann_path_occ, allow_pickle=True)\n","        processed_img_occ, bbox = process_annotation(ann_occ, img_size)\n","\n","        ann_gt = np.load(ann_path_gt, allow_pickle=True)\n","        processed_img_gt, bbox = process_annotation(ann_gt, img_size)\n","\n","        processed_img_occ = self.crop_using_bbox(processed_img_occ, bbox, img_path, img_size)\n","        processed_img_gt = self.crop_using_bbox(processed_img_gt, bbox,img_path, img_size)\n","        image = self.crop_using_bbox(np.array(image), bbox, img_path, img_size)\n","        image = resize_and_pad(image)\n","        processed_img_gt = resize_and_pad(processed_img_gt.convert(\"L\"))\n","        processed_img_occ = resize_and_pad(processed_img_occ.convert(\"L\"))\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            processed_img_occ = self.transform(processed_img_occ)\n","            processed_img_occ = processed_img_occ // torch.max(processed_img_occ)\n","            processed_img_gt = self.transform(processed_img_gt)\n","            processed_img_gt = processed_img_gt // torch.max(processed_img_gt)\n","        return {\n","            'image': image,\n","            'processed_img_occ': processed_img_occ,\n","            'processed_img_gt': processed_img_gt,\n","            'path' : ann_path_gt,\n","        }\n","    \n","    def crop_using_bbox(self, image_array, bbox, img_path, img_size):\n","        y_min, x_min, y_max, x_max = bbox\n","\n","        cropped_img = Image.fromarray(image_array[y_min:y_max, x_min:x_max])\n","        return cropped_img\n","\n","def resize_and_pad(item, target_size = (256,256), padding_value=0):\n","    original_width, original_height = item.size\n","    ratio = min(target_size[0] / original_width, target_size[1] / original_height)\n","    new_size = (int(original_width * ratio), int(original_height * ratio))\n","    \n","    interpolation_method = Image.NEAREST if item.mode == 'L' else  Image.Resampling.LANCZOS\n","    item = item.resize(new_size, interpolation_method)\n","\n","    mode = 'L' if item.mode == 'L' else 'RGB'\n","    new_item = Image.new(mode, target_size, padding_value)\n","\n","    paste_x = (target_size[0] - new_size[0]) // 2\n","    paste_y = (target_size[1] - new_size[1]) // 2\n","    new_item.paste(item, (paste_x, paste_y))\n","\n","    return new_item\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.305291Z","iopub.status.busy":"2024-04-28T14:59:44.304931Z","iopub.status.idle":"2024-04-28T14:59:44.314458Z","shell.execute_reply":"2024-04-28T14:59:44.313705Z","shell.execute_reply.started":"2024-04-28T14:59:44.305257Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### GET RANDOM MASKS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.315792Z","iopub.status.busy":"2024-04-28T14:59:44.315495Z","iopub.status.idle":"2024-04-28T14:59:44.324303Z","shell.execute_reply":"2024-04-28T14:59:44.323415Z","shell.execute_reply.started":"2024-04-28T14:59:44.315764Z"},"trusted":true},"outputs":[],"source":["def get_smasks_batch(batch = 16):\n","    path = \"/kaggle/input/good3dmasks200/kaggle/working/3dmasks\"\n","    masks = []\n","    for i in range(batch):\n","        random_mask = random.choice(list(os.listdir(path)))\n","        p = os.path.join(path, random_mask)\n","        sampled_mask = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n","        masks.append(sampled_mask)\n","    masks = np.array(masks)\n","    masks = torch.from_numpy(masks)\n","    masks = masks / 255.0\n","    \n","    return masks"]},{"cell_type":"markdown","metadata":{},"source":["### CLOSEST MASKS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.326005Z","iopub.status.busy":"2024-04-28T14:59:44.325630Z","iopub.status.idle":"2024-04-28T14:59:44.334965Z","shell.execute_reply":"2024-04-28T14:59:44.334154Z","shell.execute_reply.started":"2024-04-28T14:59:44.325973Z"},"trusted":true},"outputs":[],"source":["def load_dict(path_to_dict):\n","    with open(path_to_dict, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","# class MaskFeatureExtractor(torch.nn.Module):\n","#     def __init__(self, base_model):\n","#         super().__init__()\n","#         self.base_model = base_model\n","#         self.squeeze_layer = torch.nn.Linear(1000, 400)\n","\n","#     def forward(self, x):\n","#         x = self.base_model(x)\n","#         x = torch.flatten(x, 1)\n","#         x = self.squeeze_layer(x)\n","#         return x\n","\n","# def init_feature_extractor():\n","#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#     resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n","#     resnet.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","#     model = MaskFeatureExtractor(resnet).to(device)\n","#     model.eval()\n","#     return model\n","\n","# def get_features_batch(masks, resnet_model):\n","#     with torch.no_grad():\n","#         mask_features = resnet_model(masks)  \n","#     return mask_features\n","\n","\n","# def find_closest_masks(batch_target_features, mask_features_dict):\n","#     mask_names = list(mask_features_dict.keys())\n","#     mask_features_list = [mask_features_dict[name] for name in mask_names]\n","#     mask_features = torch.stack([torch.tensor(features, dtype=batch_target_features.dtype, device=batch_target_features.device)\n","#                                  if not isinstance(features, torch.Tensor) else features\n","#                                  for features in mask_features_list])\n","#     cosine_sim = F.cosine_similarity(batch_target_features.unsqueeze(1), mask_features.unsqueeze(0), dim=2)\n","#     cosine_distance = 1 - cosine_sim\n","#     min_distance_indices = torch.argmin(cosine_distance, dim=1)\n","#     closest_masks = [mask_names[idx] for idx in min_distance_indices]\n","#     return closest_masks\n","\n","# def find_closest_mask(target_features, mask_features_dict):\n","#     closest_mask = None\n","#     min_distance = float('inf')  # Start with the highest possible distance\n","\n","#     for mask_name, features in mask_features_dict.items():\n","\n","#         if not isinstance(features, torch.Tensor):\n","#             features = torch.tensor(features, dtype=target_features.dtype, device=target_features.device)\n","#         features = features.unsqueeze(0)  # Add batch dimension\n","#         cosine_sim = F.cosine_similarity(target_features, features, dim=1).item()\n","#         cosine_distance = 1 - cosine_sim  # Calculate cosine distance\n","#         if cosine_distance < min_distance:\n","#             min_distance = cosine_distance\n","#             closest_mask = mask_name\n","\n","#     return closest_mask\n","\n","\n","# def find_closest_masks_batch(current_features, vocab):\n","#     res = []\n","#     for feature in current_features:\n","#         closest_mask = find_closest_mask(feature, vocab)\n","#         res.append(closest_mask)\n","#     return r\n","\n","# def find_closest_masks_batch(target_features, mask_features_dict):\n","#     for mask_name, features in mask_features_dict.items():\n","#         if not isinstance(features, torch.Tensor):\n","#             mask_features_dict[mask_name] = torch.tensor(features, dtype=target_features.dtype, device=target_features.device)\n","\n","#     all_mask_features = torch.stack(list(mask_features_dict.values()))\n","#     all_mask_names = list(mask_features_dict.keys())\n","\n","#     cosine_sim = F.cosine_similarity(target_features.unsqueeze(1), all_mask_features.unsqueeze(0), dim=2)\n","#     cosine_distances = 1 - cosine_sim  # Convert similarity to distance\n","\n","#     min_indices = torch.argmin(cosine_distances, dim=1)\n","\n","#     closest_masks = [all_mask_names[idx] for idx in min_indices]\n","#     return closest_masks\n","\n","\n","# def get_closest_masks(loader, resnet_model, vocab):\n","#     mapping = {}\n","#     for data in tqdm(loader):\n","#         _, _, amodal_mask, image_name = data[\"image\"], data[\"processed_img_occ\"],\\\n","#                                             data[\"processed_img_gt\"], data[\"path\"]\n","#         current_features = get_features_batch(amodal_mask.to(\"cuda\"), resnet_model)\n","#         closest_masks = find_closest_masks_batch(current_features, vocab)\n","#         for n, m in zip(image_name, closest_masks):\n","#             mapping[n] = m\n","#     return mapping\n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Contours"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.339095Z","iopub.status.busy":"2024-04-28T14:59:44.338738Z","iopub.status.idle":"2024-04-28T14:59:44.365227Z","shell.execute_reply":"2024-04-28T14:59:44.364165Z","shell.execute_reply.started":"2024-04-28T14:59:44.339070Z"},"trusted":true},"outputs":[],"source":["def smooth_contour(contour, epsilon_factor=0.0001):\n","    epsilon = epsilon_factor * cv2.arcLength(contour, True)\n","    smoothed_contour = cv2.approxPolyDP(contour, epsilon, True)\n","    return smoothed_contour\n","\n","def get_contour(mask):\n","    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cont = max(contours, key=cv2.contourArea)\n","    cont = smooth_contour(cont)\n","    return cont\n","\n","def find_bounding_box(mask, increase_by=0.5):\n","    rows = np.any(mask, axis=1)\n","    cols = np.any(mask, axis=0)\n","    ymin, ymax = np.where(rows)[0][[0, -1]]\n","    xmin, xmax = np.where(cols)[0][[0, -1]]\n","    x_center = (xmin + xmax) / 2\n","    y_center = (ymin + ymax) / 2\n","    new_width = (xmax - xmin) * (1 + increase_by)\n","    new_height = (ymax - ymin) * (1 + increase_by)\n","    xmin = max(0, int(x_center - new_width / 2))\n","    xmax = int(x_center + new_width / 2)\n","    ymin = max(0, int(y_center - new_height / 2))\n","    ymax = int(y_center + new_height / 2)\n","    \n","    return xmin, ymin, xmax, ymax\n","\n","\n","def crop_and_center_mask(mask, output_size=(256,256)):\n","    xmin, ymin, xmax, ymax = find_bounding_box(mask, increase_by=0.5)\n","    ymax = min(ymax, mask.shape[0])\n","    xmax = min(xmax, mask.shape[1])\n","    cropped_mask = mask[ymin:ymax, xmin:xmax]\n","    \n","    if output_size is None:\n","        return cropped_mask\n","    centered_mask = np.zeros((output_size[1], output_size[0]), dtype=np.uint8)\n","    x_offset = (output_size[0] - cropped_mask.shape[1]) // 2\n","    y_offset = (output_size[1] - cropped_mask.shape[0]) // 2\n","    centered_mask[y_offset:y_offset+cropped_mask.shape[0], x_offset:x_offset+cropped_mask.shape[1]] = cropped_mask\n","    \n","    return centered_mask\n","    \n","def compare_masks(mask1, mask2):\n","    mask1_processed = preprocess_mask(mask1)\n","    mask2_processed = preprocess_mask(mask2)\n","\n","    contours1, _ = cv2.findContours(mask1_processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    contours2, _ = cv2.findContours(mask2_processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    cnt1 = max(contours1, key=cv2.contourArea)\n","    cnt2 = max(contours2, key=cv2.contourArea)\n","\n","    match = cv2.matchShapes(cnt1, cnt2, 1, 0.0)\n","    return match\n","\n","def get_best(current_c, vocab):\n","    best_m = float(\"inf\")\n","    best_p = \"\"\n","    for key in vocab.keys():\n","        match = cv2.matchShapes(current_c, vocab[key], 1, 0.0)\n","        if match < best_m:\n","            best_m = match\n","            best_p = key\n","    return best_p\n","\n","def get_best_batch(amodal_masks, vocab):\n","    best_masks = []\n","    for mask in amodal_masks:\n","        mask = mask.cpu().numpy()\n","        mask = mask[0].astype(np.uint8)\n","        mask = crop_and_center_mask(mask)\n","\n","        mask_c = get_contour(mask)\n","        best_mask = get_best(mask_c, vocab)\n","        best_masks.append(best_mask)\n","    return best_masks\n","\n","\n","def draw_contours(contours, canvas_size, contour_color=(0, 255, 0), thickness=2):\n","    canvas = np.zeros((canvas_size[1], canvas_size[0], 3), dtype=np.uint8)\n","    \n","    cv2.drawContours(canvas, contours, -1, contour_color, thickness)\n","    plt.imshow(canvas)\n","    plt.show()\n","\n","    return canvas\n","\n","\n","def get_closest_by_c(names):\n","    res = []\n","    for name in names:\n","        img = cv2.imread(name, cv2.IMREAD_GRAYSCALE)\n","        res.append(img)\n","    return res\n","\n","def create_mapping(loader, vocab):\n","    mapping = {}\n","    for data in tqdm(loader, desc = \"Create mapping\"):\n","        input_image, inmodal_mask, amodal_mask, names = data[\"image\"],\\\n","                            data[\"processed_img_occ\"], data[\"processed_img_gt\"], data[\"path\"]\n","        best_names = get_best_batch(amodal_mask, vocab)\n","        for i in range(len(names)):\n","            mapping[names[i]] = cv2.imread(best_names[i], cv2.IMREAD_GRAYSCALE)\n","    return mapping\n","\n","def get_sampled_batch(names, mapping, pascal=False):\n","    res = torch.Tensor()\n","    for name in names:\n","        if pascal:\n","            parts = name.split(\"/\")\n","            last = parts[-1].split(\".\")[0]\n","            last = \".\".join([last, \"jpeg\"])  \n","            parts[-1] = last\n","            name = \"/\".join(parts)\n","            name = name.replace(\"anns\", \"images\")\n","        mapped = mapping[name]  \n","        res = torch.cat((res, torch.from_numpy(mapped).unsqueeze(0)))\n","    return res\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["### LOSSES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.366751Z","iopub.status.busy":"2024-04-28T14:59:44.366443Z","iopub.status.idle":"2024-04-28T14:59:44.381266Z","shell.execute_reply":"2024-04-28T14:59:44.380410Z","shell.execute_reply.started":"2024-04-28T14:59:44.366727Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.382688Z","iopub.status.busy":"2024-04-28T14:59:44.382391Z","iopub.status.idle":"2024-04-28T14:59:44.410723Z","shell.execute_reply":"2024-04-28T14:59:44.409835Z","shell.execute_reply.started":"2024-04-28T14:59:44.382643Z"},"trusted":true},"outputs":[],"source":["class PerceptualLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n","        super(PerceptualLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        inputs = torch.clamp(inputs, min=1e-7, max=1 - 1e-7)\n","        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.where(targets == 1, inputs, 1 - inputs)\n","        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n","        \n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","    \n","\n","class GradLayer(nn.Module):\n","    def __init__(self):\n","        super(GradLayer, self).__init__()\n","        kernel_v = torch.tensor([[0, -1, 0], [0, 0, 0], [0, 1, 0]], dtype=torch.float32).view(1, 1, 3, 3)\n","        kernel_h = torch.tensor([[0, 0, 0], [-1, 0, 1], [0, 0, 0]], dtype=torch.float32).view(1, 1, 3, 3)\n","        self.weight_h = nn.Parameter(data=kernel_h, requires_grad=False)\n","        self.weight_v = nn.Parameter(data=kernel_v, requires_grad=False)\n","\n","    def forward(self, x):\n","        x_v = F.conv2d(x, self.weight_v, padding=1)\n","        x_h = F.conv2d(x, self.weight_h, padding=1)\n","        gradients = torch.sqrt(x_v.pow(2) + x_h.pow(2) + 1e-6)\n","        return gradients\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n","\n","        if self.reduction == 'mean':\n","            return torch.mean(F_loss)\n","        elif self.reduction == 'sum':\n","            return torch.sum(F_loss)\n","        else:\n","            return F_loss\n","\n","\n","class EdgeRefineLoss(nn.Module):\n","    def __init__(self, alpha=0.5, beta=10.0, gamma=2, focal_alpha=0.25):\n","        super(EdgeRefineLoss, self).__init__()\n","        self.grad_layer = GradLayer()\n","        self.grad_loss = nn.L1Loss()\n","        self.focal_loss = FocalLoss(gamma=gamma, alpha=focal_alpha)\n","        self.alpha = alpha  \n","        self.beta = beta    \n","\n","    def forward(self, predicted, gt_full_mask, gt_visible_mask):\n","        \n","        predicted_edges = self.grad_layer(predicted)\n","        target_full_edges = self.grad_layer(gt_full_mask)\n","        seg_loss_full = self.focal_loss(predicted, gt_full_mask)\n","        gt_invisible_mask = gt_full_mask - gt_visible_mask\n","        predicted_invisible = predicted * (gt_invisible_mask > 0).float()\n","        seg_loss_invisible = self.focal_loss(predicted_invisible, gt_invisible_mask)\n","        edge_loss = self.grad_loss(predicted_edges, target_full_edges)\n","        total_loss = self.alpha * edge_loss + self.beta * (0.5 * seg_loss_full + 0.5 * seg_loss_invisible)\n","        return total_loss\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.412074Z","iopub.status.busy":"2024-04-28T14:59:44.411754Z","iopub.status.idle":"2024-04-28T14:59:44.421802Z","shell.execute_reply":"2024-04-28T14:59:44.420992Z","shell.execute_reply.started":"2024-04-28T14:59:44.412042Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.423580Z","iopub.status.busy":"2024-04-28T14:59:44.423044Z","iopub.status.idle":"2024-04-28T14:59:44.437963Z","shell.execute_reply":"2024-04-28T14:59:44.437147Z","shell.execute_reply.started":"2024-04-28T14:59:44.423549Z"},"trusted":true},"outputs":[],"source":["def smooth_positive_labels(y):\n","    y = torch.clamp(y - 0.1, min=0.0)\n","    random_part = torch.rand(y.shape, device=y.device) * 0.1\n","    y = torch.clamp(y + random_part, max=1.0)\n","    return y\n","\n","def flip_labels(labels, flip_prob=0.05):\n","    flip_mask = torch.rand(labels.shape, device=labels.device) < flip_prob\n","    flipped_labels = torch.where(flip_mask, 1 - labels, labels)\n","    flipped_labels = torch.clamp(flipped_labels, min=0.0, max=1.0)\n","    return flipped_labels\n","\n","\n","class OBJ_D_loss(nn.Module):\n","    def __init__(self, flip_prob=0.1):\n","        super(OBJ_D_loss, self).__init__()\n","        self.bce_loss = torch.nn.BCELoss()\n","        self.flip_prob = flip_prob\n","\n","    def forward(self, pred_real_gt, pred_fake , pred_real_s):\n","        real_labels = smooth_positive_labels(torch.full_like(pred_real_gt, 1.0))\n","        fake_labels = torch.full_like(pred_fake, 0.0)\n","\n","        real_labels = flip_labels(real_labels, self.flip_prob)\n","        fake_labels = flip_labels(fake_labels, self.flip_prob)\n","\n","        loss_real_gt = self.bce_loss(pred_real_gt, real_labels)\n","        loss_real_s = self.bce_loss(pred_real_s, real_labels)\n","        loss_fake = self.bce_loss(pred_fake, fake_labels)\n","\n","        loss_real = (loss_real_gt + loss_real_s) / 2\n","        return loss_real + loss_fake\n","\n","class INS_D_loss(nn.Module):\n","    def __init__(self, flip_prob=0.1):\n","        super(INS_D_loss, self).__init__()\n","        self.bce_loss = torch.nn.BCELoss()\n","        self.flip_prob = flip_prob\n","\n","    def forward(self, pred_real_gt, pred_fake_G_output, pred_fake_M_s):\n","        real_labels = smooth_positive_labels(torch.full_like(pred_real_gt, 1.0))\n","        fake_labels = torch.full_like(pred_fake_G_output, 0.0)\n","\n","        real_labels = flip_labels(real_labels, self.flip_prob)\n","        fake_labels = flip_labels(fake_labels, self.flip_prob)\n","\n","        loss_real = self.bce_loss(pred_real_gt, real_labels)\n","        loss_fake_G_output = self.bce_loss(pred_fake_G_output, fake_labels)\n","        loss_fake_M_s = self.bce_loss(pred_fake_M_s, fake_labels)\n","\n","        loss_fake = (loss_fake_G_output + loss_fake_M_s) / 2\n","        return loss_real + loss_fake\n","\n","class SegmentationLoss(nn.Module):\n","    def __init__(self, lambda_val, beta_val, perceptual_loss_fn):\n","        super(SegmentationLoss, self).__init__()\n","        self.lambda_val = lambda_val\n","        self.beta_val = beta_val\n","        self.l1_loss_fn = nn.L1Loss()\n","\n","    def forward(self, G_output, M_gt, obj_loss, ins_loss, perceptual_loss):\n","        l1_loss = self.l1_loss_fn(G_output, M_gt)\n","        return self.lambda_val * l1_loss + self.beta_val * perceptual_loss + obj_loss + ins_loss\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### TRAIN EPOCH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.440106Z","iopub.status.busy":"2024-04-28T14:59:44.439341Z","iopub.status.idle":"2024-04-28T14:59:44.456931Z","shell.execute_reply":"2024-04-28T14:59:44.456053Z","shell.execute_reply.started":"2024-04-28T14:59:44.440073Z"},"trusted":true},"outputs":[],"source":["def train_epoch(train_loader, generator, d_obj, d_ins, egde_refine_module,\n","                device, optim_g, optim_d_obj, optim_d_ins, optim_edge_refine,\n","                segmentation_loss, obj_d_loss_fn, ins_d_loss_fn,\n","                perceptual_loss_fn, edge_refine_loss_fn, epoch, save_folder, mapping):\n","    total_d_obj_loss = 0\n","    total_d_ins_loss = 0\n","    total_seg_loss = 0\n","    total_perceptual = 0\n","    total_edge_refine_loss = 0\n","    \n","    num_batches = len(train_loader)\n","    print(\"len map in train \", len(mapping))\n","\n","    for i, data in enumerate(tqdm(train_loader)):\n","        input_image, inmodal_mask, amodal_mask, names = data[\"image\"], data[\"processed_img_occ\"], data[\"processed_img_gt\"], data[\"path\"]\n","        input_image, inmodal_mask, amodal_mask = input_image.to(device), inmodal_mask.to(device), amodal_mask.to(device)\n","        input_tensor = torch.cat((input_image, inmodal_mask), dim=1).to(device)\n","\n","        sampled_masks = get_smasks_batch().unsqueeze(dim=1).to(device)\n","        sampled_masks_obj = get_sampled_batch(names, mapping).unsqueeze(1).to(device)\n","\n","        # Generator forward pass\n","        fake_masks = generator(input_tensor)\n","        \n","        fake_masks_to_refine = fake_masks.clone().detach()\n","        fake_masks_to_refine = torch.where(fake_masks_to_refine < 0.8, torch.zeros_like(fake_masks_to_refine), torch.ones_like(fake_masks_to_refine))\n","        refine_input = torch.cat((inmodal_mask, input_image, fake_masks_to_refine), dim = 1).to(device)\n","#         print(refine_input.shape)\n","        refined_masks = egde_refine_module(refine_input)\n","\n","        # Discriminator predictions\n","        d_obj_pred_real = d_obj(amodal_mask)\n","        d_obj_pred_fake = d_obj(fake_masks.detach())\n","        d_obj_pred_sampled = d_obj(sampled_masks_obj)\n","        \n","        d_ins_pred_real = d_ins(torch.cat((amodal_mask, input_image, inmodal_mask), dim=1))\n","        d_ins_pred_fake = d_ins(torch.cat((fake_masks.detach(), input_image, inmodal_mask), dim=1))\n","        d_ins_pred_sampled = d_ins(torch.cat((sampled_masks, input_image, inmodal_mask), dim=1))\n","        \n","        # Discriminator OBJ loss and update\n","        optim_d_obj.zero_grad()\n","        d_obj_loss = obj_d_loss_fn(d_obj_pred_real, d_obj_pred_fake, d_obj_pred_sampled)\n","#         print(\"d_obj_loss = \", d_obj_loss)\n","        d_obj_loss.backward()\n","        optim_d_obj.step()\n","\n","        # Discriminator INS loss and update\n","        optim_d_ins.zero_grad()\n","        d_ins_loss = ins_d_loss_fn(d_ins_pred_real, d_ins_pred_fake, d_ins_pred_sampled)\n","        d_ins_loss.backward()\n","        optim_d_ins.step()\n","\n","        # Generator loss and update\n","        optim_g.zero_grad()\n","        perceptual = perceptual_loss_fn(fake_masks, amodal_mask)\n","        \n","        seg_loss = segmentation_loss(fake_masks, amodal_mask, d_obj_loss.item(), d_ins_loss.item(), perceptual.item())\n","        seg_loss.backward()\n","        optim_g.step()\n","        \n","        optim_edge_refine.zero_grad()\n","        edge_refine_loss = edge_refine_loss_fn(refined_masks, amodal_mask, inmodal_mask)\n","        edge_refine_loss.backward()\n","        optim_edge_refine.step()\n","\n","        total_d_obj_loss += d_obj_loss.item()\n","        total_d_ins_loss += d_ins_loss.item()\n","        total_seg_loss += seg_loss.item()\n","        total_perceptual += perceptual.item()\n","        total_edge_refine_loss += edge_refine_loss.item()\n","\n","        if i == num_batches - 1 and epoch % 5 == 0:\n","            save_images_with_overlay(input_image, fake_masks,refined_masks,amodal_mask, save_folder, epoch)\n","\n","            \n","\n","    avg_d_obj_loss = total_d_obj_loss / num_batches\n","    avg_d_ins_loss = total_d_ins_loss / num_batches\n","    avg_seg_loss = total_seg_loss / num_batches\n","    avg_perceptual = total_perceptual / num_batches\n","    avg_edge_refine_loss = total_edge_refine_loss / num_batches\n","    return avg_d_obj_loss, avg_d_ins_loss, avg_seg_loss, avg_perceptual, avg_edge_refine_loss\n"]},{"cell_type":"markdown","metadata":{},"source":["### VALIDATION EPOCH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.458792Z","iopub.status.busy":"2024-04-28T14:59:44.458282Z","iopub.status.idle":"2024-04-28T14:59:44.490350Z","shell.execute_reply":"2024-04-28T14:59:44.489469Z","shell.execute_reply.started":"2024-04-28T14:59:44.458761Z"},"trusted":true},"outputs":[],"source":["def validate_epoch(val_loader, generator, d_obj, d_ins, edge_refine_module,\n","                   device, segmentation_loss, perceptual_loss_fn,\n","                   obj_d_loss_fn, ins_d_loss_fn, edge_refine_loss_fn, epoch, save_folder,\n","                mapping):\n","    \n","    total_d_obj_loss = 0\n","    total_d_ins_loss = 0\n","    total_seg_loss = 0\n","    total_iou = 0\n","    total_iou_r = 0\n","    total_precision = 0\n","    total_precision_r = 0\n","    total_recall = 0\n","    total_recall_r = 0\n","    total_f1_r = 0\n","    total_f1 = 0\n","    total_l1_error = 0\n","    total_l2_error = 0\n","    total_edge_refine_loss = 0\n","\n","    generator.eval()\n","    d_obj.eval()\n","    d_ins.eval()\n","    edge_refine_module.eval()\n","\n","    with torch.no_grad():\n","        num_batches = len(val_loader)\n","        for i, data in enumerate(tqdm(val_loader, desc=\"Validation\")):\n","            input_image, inmodal_mask, amodal_mask, names = data[\"image\"], data[\"processed_img_occ\"], data[\"processed_img_gt\"], data[\"path\"]\n","            input_image, inmodal_mask, amodal_mask = input_image.to(device), inmodal_mask.to(device), amodal_mask.to(device)\n","            input_tensor = torch.cat((input_image, inmodal_mask), dim=1).to(device)\n","            \n","            fake_masks = generator(input_tensor)\n","            \n","            fake_masks_to_refine = fake_masks.clone().detach()\n","            fake_masks_to_refine = torch.where(fake_masks_to_refine < 0.8, torch.zeros_like(fake_masks_to_refine), torch.ones_like(fake_masks_to_refine))\n","            refine_input = torch.cat((inmodal_mask, input_image, fake_masks_to_refine), dim = 1).to(device)\n","            refined_masks = edge_refine_module(refine_input)\n","            \n","            sampled_masks = get_smasks_batch().unsqueeze(dim=1).to(device)\n","#             sampled_masks_obj = get_closest_mask_obj(amodal_mask, masks_dict, resnet_model, mask_images).to(device)\n","#             sampled_masks_obj = get_closest_masks_mapping(mapping, names)\n","            sampled_masks_obj = get_sampled_batch(names, mapping).unsqueeze(1).to(device)\n","\n","\n","            precision, recall, f1, iou = compute_segmentation_metrics(fake_masks, amodal_mask)\n","            precision_r, recall_r, f1_r, iou_r = compute_segmentation_metrics(refined_masks, amodal_mask)\n","            \n","            l1_error = torch.nn.functional.l1_loss(fake_masks, amodal_mask).item()\n","            l2_error = torch.nn.functional.mse_loss(fake_masks, amodal_mask).item()\n","\n","            d_obj_pred_real = d_obj(amodal_mask)\n","            d_obj_pred_fake = d_obj(fake_masks)\n","            d_obj_pred_sampled = d_obj(sampled_masks_obj)\n","            \n","            d_ins_pred_real = d_ins(torch.cat((amodal_mask, input_image, inmodal_mask), dim=1))\n","            d_ins_pred_fake = d_ins(torch.cat((fake_masks, input_image, inmodal_mask), dim=1))\n","            d_ins_pred_sampled = d_ins(torch.cat((sampled_masks, input_image, inmodal_mask), dim=1))\n","\n","            d_obj_loss = obj_d_loss_fn(d_obj_pred_real, d_obj_pred_fake, d_obj_pred_sampled)\n","            d_ins_loss = ins_d_loss_fn(d_ins_pred_real, d_ins_pred_fake, d_ins_pred_sampled)\n","            perceptual_loss = perceptual_loss_fn(fake_masks, amodal_mask)\n","            \n","#             print(\"contout_consistency loss = \", contour * 0.25)\n","            seg_loss = segmentation_loss(fake_masks, amodal_mask, d_obj_loss.item(), d_ins_loss.item(), perceptual_loss.item())\n","            edge_refine_loss = edge_refine_loss_fn(refined_masks, amodal_mask, inmodal_mask)\n","\n","            total_d_obj_loss += d_obj_loss.item()\n","            total_d_ins_loss += d_ins_loss.item()\n","            total_seg_loss += seg_loss.item()\n","            total_iou += iou\n","            total_iou_r += iou_r\n","            total_precision += precision\n","            total_precision_r += precision_r\n","            total_recall += recall\n","            total_recall_r += recall_r\n","            total_f1 += f1\n","            total_f1_r += f1_r\n","            total_l1_error += l1_error\n","            total_l2_error += l2_error\n","            total_edge_refine_loss += edge_refine_loss.item()\n","            \n","            if i == num_batches - 1 and epoch % 5 == 0:\n","                save_images_with_overlay(input_image, fake_masks,refined_masks,amodal_mask, save_folder, epoch)\n","\n","    avg_metrics = {\n","        \"d_obj_loss\": total_d_obj_loss / num_batches,\n","        \"d_ins_loss\": total_d_ins_loss / num_batches,\n","        \"seg_loss\": total_seg_loss / num_batches,\n","        \"iou\": total_iou / num_batches,\n","        \"precision\": total_precision / num_batches,\n","        \"recall\": total_recall / num_batches,\n","        \"f1\": total_f1 / num_batches,\n","        \"l1_error\": total_l1_error / num_batches,\n","        \"l2_error\": total_l2_error / num_batches,\n","        \"refine_module_loss\": total_edge_refine_loss / num_batches,\n","        \"iou_r\":  total_iou_r / num_batches,\n","        \"precision_r\": total_precision_r / num_batches,\n","        \"recall_r\": total_recall_r / num_batches,\n","        \"f1_r\": total_f1_r / num_batches,\n","        \n","    }\n","\n","    return avg_metrics\n","\n","def compute_segmentation_metrics(pred, target, threshold=0.8):\n","    pred = (pred > threshold).float()\n","    pred = pred.view(-1)\n","    target = target.view(-1)\n","    true_positive = (pred * target).sum()\n","    false_positive = (pred * (1 - target)).sum()\n","    false_negative = ((1 - pred) * target).sum()\n","    precision = true_positive / (true_positive + false_positive + 1e-8)\n","    recall = true_positive / (true_positive + false_negative + 1e-8)\n","    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n","    intersection = true_positive\n","    union = true_positive + false_positive + false_negative\n","    iou = intersection / (union + 1e-8)\n","\n","    return precision.item(), recall.item(), f1_score.item(), iou.item()\n","\n","\n","\n","def overlay_masks(input_images, masks, alpha=0.3, color=[1.0, 0.0, 0.0]):  \n","    masks_rgb = masks.repeat(1, 3, 1, 1) * torch.tensor(color, device=masks.device).view(1, 3, 1, 1)\n","    overlayed_images = input_images.clone()  \n","    overlayed_images = overlayed_images * (1 - alpha) + masks_rgb * alpha\n","    return overlayed_images\n","\n","def save_images_with_overlay(input_images, fake_masks, refined_masks, amodal_masks, folder, epoch):\n","    overlayed_fake = overlay_masks(input_images, fake_masks, alpha=0.3, color=[1.0, 0.0, 0.0])\n","    overlayed_amodal = overlay_masks(input_images, amodal_masks, alpha=0.3, color=[0.0, 0.0, 1.0])\n","    overlayed_refined = overlay_masks(input_images, refined_masks, alpha=0.3, color=[0.0, 0.0, 1.0])\n","    \n","\n","    os.makedirs(folder, exist_ok=True)\n","    for i, (img_fake, img_amodal, img_refined) in enumerate(zip(overlayed_fake, overlayed_amodal, overlayed_refined)):\n","        image_fake = to_pil_image(img_fake.cpu()).convert(\"RGB\")\n","        image_refined = to_pil_image(img_refined.cpu()).convert(\"RGB\")\n","        filename_fake = os.path.join(folder, f\"fake_{i}_{epoch}.png\")\n","        filename_refined = os.path.join(folder, f\"refined_{i}_{epoch}.png\")\n","        image_fake.save(filename_fake)\n","        image_refined.save(filename_refined)\n","        if epoch == 5:\n","            image_amodal = to_pil_image(img_amodal.cpu()).convert(\"RGB\")\n","            filename_amodal = os.path.join(folder, f\"amodal_{i}_{epoch}.png\")\n","            image_amodal.save(filename_amodal)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.492119Z","iopub.status.busy":"2024-04-28T14:59:44.491862Z","iopub.status.idle":"2024-04-28T14:59:44.514947Z","shell.execute_reply":"2024-04-28T14:59:44.513942Z","shell.execute_reply.started":"2024-04-28T14:59:44.492098Z"},"trusted":true},"outputs":[],"source":["def train_and_validate(train_loader, val_loader, d_obj, d_ins, generator,edge_refine_module, mapping):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"\\nAvailable device: {device}\")\n","    optim_d_obj = Adam(d_obj.parameters(), lr=1e-5, betas=(0.5, 0.999))\n","    optim_d_ins = Adam(d_ins.parameters(), lr=1e-5, betas=(0.5, 0.999))\n","    optim_g = Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n","    optim_refine =  Adam(edge_refine_module.parameters(), lr=1e-4, betas=(0.5, 0.999))\n","\n","    scheduler_d_obj = ReduceLROnPlateau(optim_d_obj, mode='min', factor=0.5, patience=15, verbose=True, min_lr = 1e-7)\n","    scheduler_d_ins = ReduceLROnPlateau(optim_d_ins, mode='min', factor=0.5, patience=15, verbose=True, min_lr = 1e-7)\n","    scheduler_g = ReduceLROnPlateau(optim_g, mode='min', factor=0.5, patience=15, verbose=True, min_lr = 1e-7)\n","    scheduler_refine = ReduceLROnPlateau(optim_refine, mode='min', factor=0.5, patience=10, verbose=True, min_lr = 1e-7)\n","\n","    perceptual_loss_fn = PerceptualLoss().to(device)\n","    segmentation_loss = SegmentationLoss(20, 1, perceptual_loss_fn).to(device)\n","    obj_d_loss_fn = OBJ_D_loss().to(device)\n","    ins_d_loss_fn = INS_D_loss().to(device)\n","    refine_loss_fn = EdgeRefineLoss().to(device)\n","    \n","    metrics = {\n","        'train_d_obj_loss': [],\n","        'train_d_ins_loss': [],\n","        'train_seg_loss': [],\n","        'val_d_obj_loss': [],\n","        'val_d_ins_loss': [],\n","        'val_seg_loss': [],\n","        'val_iou': [],\n","        'val_precision': [],\n","        'val_recall': [],\n","        'val_f1': []\n","    }\n","\n","    epochs = 100\n","    for epoch in range(epochs):\n","        train_losses = train_epoch(train_loader, generator, d_obj, d_ins, edge_refine_module, device, \n","                                   optim_g, optim_d_obj, optim_d_ins,optim_refine, segmentation_loss, \n","                                   obj_d_loss_fn, ins_d_loss_fn, perceptual_loss_fn,refine_loss_fn, epoch,\n","                                   \"/kaggle/working/training_output\", mapping)\n","\n","        validation_losses = validate_epoch(val_loader, generator,\n","                                           d_obj, d_ins,edge_refine_module,  device, segmentation_loss,\n","                                           perceptual_loss_fn, obj_d_loss_fn,\n","                                           ins_d_loss_fn,refine_loss_fn, epoch,\n","                                           \"/kaggle/working/validation_output\",\n","                                            mapping)\n","        if epoch % 10 == 0 and epoch != 0:\n","            torch.save(generator.state_dict(), 'model_weights_gen.pth')\n","            torch.save(edge_refine_module.state_dict(), 'edge_refine_weights.pth')\n","        print(f\"Epoch {epoch} train losses = \", train_losses)\n","        print(f\"Epoch {epoch} validation losses = \", validation_losses)\n","        scheduler_d_obj.step(validation_losses[\"d_obj_loss\"])\n","        scheduler_d_ins.step(validation_losses[\"d_ins_loss\"])\n","        scheduler_g.step(validation_losses[\"seg_loss\"])\n","        scheduler_refine.step(validation_losses[\"refine_module_loss\"])\n","\n","        metrics['train_d_obj_loss'].append(train_losses[0])\n","        metrics['train_d_ins_loss'].append(train_losses[1])\n","        metrics['train_seg_loss'].append(train_losses[2])\n","        metrics['val_d_obj_loss'].append(validation_losses['d_obj_loss'])\n","        metrics['val_d_ins_loss'].append(validation_losses['d_ins_loss'])\n","        metrics['val_seg_loss'].append(validation_losses['seg_loss'])\n","        metrics['val_iou'].append(validation_losses['iou'])\n","        metrics['val_precision'].append(validation_losses['precision'])\n","        metrics['val_recall'].append(validation_losses['recall'])\n","        metrics['val_f1'].append(validation_losses['f1'])\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.subplot(3, 2, 1)\n","    plt.plot(metrics['train_d_obj_loss'], label='Train Discriminator Obj Loss')\n","    plt.plot(metrics['val_d_obj_loss'], label='Val Discriminator Obj Loss')\n","    plt.title('Discriminator Obj Loss')\n","    plt.legend()\n","\n","    plt.subplot(3, 2, 2)\n","    plt.plot(metrics['train_d_ins_loss'], label='Train Discriminator Ins Loss')\n","    plt.plot(metrics['val_d_ins_loss'], label='Val Discriminator Ins Loss')\n","    plt.title('Discriminator Ins Loss')\n","    plt.legend()\n","\n","    plt.subplot(3, 2, 3)\n","    plt.plot(metrics['train_seg_loss'], label='Train Segmentation Loss')\n","    plt.plot(metrics['val_seg_loss'], label='Val Segmentation Loss')\n","    plt.title('Segmentation Loss')\n","    plt.legend()\n","\n","    plt.subplot(3, 2, 4)\n","    plt.plot(metrics['val_iou'], label='Val IOU')\n","    plt.title('IOU')\n","    plt.legend()\n","\n","    plt.subplot(3, 2, 5)\n","    plt.plot(metrics['val_precision'], label='Val Precision')\n","    plt.plot(metrics['val_recall'], label='Val Recall')\n","    plt.plot(metrics['val_f1'], label='Val F1 Score')\n","    plt.title('Precision, Recall, F1 Score')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.516428Z","iopub.status.busy":"2024-04-28T14:59:44.516109Z","iopub.status.idle":"2024-04-28T14:59:44.546413Z","shell.execute_reply":"2024-04-28T14:59:44.545766Z","shell.execute_reply.started":"2024-04-28T14:59:44.516397Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# kins_dataset = KINSDataset(\"/kaggle/input/kins-data-correct/kaggle/working/kins_data\",transform = transform)\n","\n","# total_count = len(kins_dataset)\n","# train_count = int(int(0.4 * total_count) + 16 - (int(0.4 * total_count) % 16))\n","# val_count = int(int(0.2 * total_count) + 16 - (int(0.2 * total_count)  % 16))\n","# rest_count = total_count - train_count - val_count\n","\n","# # Split the dataset\n","# ds_train_kins, ds_val_kins, _ = random_split(kins_dataset, [train_count, val_count, rest_count])\n","\n","# print(len(ds_train_kins))\n","\n","# train_loader = DataLoader(ds_train_kins, batch_size=16, shuffle=True, drop_last=False, num_workers = 4)\n","# val_loader = DataLoader(ds_val_kins, batch_size=16, drop_last=False, num_workers = 4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### PASCAL LOADERS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.548312Z","iopub.status.busy":"2024-04-28T14:59:44.547551Z","iopub.status.idle":"2024-04-28T14:59:44.567096Z","shell.execute_reply":"2024-04-28T14:59:44.566146Z","shell.execute_reply.started":"2024-04-28T14:59:44.548279Z"},"trusted":true},"outputs":[],"source":["seed = 42\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.568341Z","iopub.status.busy":"2024-04-28T14:59:44.568022Z","iopub.status.idle":"2024-04-28T14:59:44.592275Z","shell.execute_reply":"2024-04-28T14:59:44.591599Z","shell.execute_reply.started":"2024-04-28T14:59:44.568311Z"},"trusted":true},"outputs":[],"source":["ds_train = OccludedVehiclesDataset(all_train_images, transform=transform)\n","ds_val = OccludedVehiclesDataset(all_val_images, transform=transform)\n","\n","train_dataset, _ = random_split(ds_train, [0.15,0.85])\n","val_dataset, _ = random_split(ds_val, [0.3, 0.7])\n","\n","#     remaining_size = dataset_size - train_size - val_size\n","#     train_dataset, val_dataset, _ = random_split(dataset, [train_size, val_size, remaining_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True, num_workers = 4)\n","val_loader = DataLoader(val_dataset, batch_size=16, drop_last=True, num_workers = 4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.598625Z","iopub.status.busy":"2024-04-28T14:59:44.598346Z","iopub.status.idle":"2024-04-28T14:59:44.605141Z","shell.execute_reply":"2024-04-28T14:59:44.604361Z","shell.execute_reply.started":"2024-04-28T14:59:44.598603Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# from IPython.display import FileLink\n","# FileLink(r'mapping.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:44.606448Z","iopub.status.busy":"2024-04-28T14:59:44.606195Z","iopub.status.idle":"2024-04-28T14:59:48.480781Z","shell.execute_reply":"2024-04-28T14:59:48.479970Z","shell.execute_reply.started":"2024-04-28T14:59:44.606425Z"},"trusted":true},"outputs":[],"source":["vocab = load_dict(\"/kaggle/input/masks200contours/my_dict-6.pkl\")\n","# val_mapping = create_mapping(val_loader, vocab)\n","# train_mapping = create_mapping(train_loader, vocab)\n","mapping = load_dict(\"/kaggle/input/pascal-mapping/pascal_mapping.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:48.482247Z","iopub.status.busy":"2024-04-28T14:59:48.481916Z","iopub.status.idle":"2024-04-28T14:59:48.486911Z","shell.execute_reply":"2024-04-28T14:59:48.485947Z","shell.execute_reply.started":"2024-04-28T14:59:48.482213Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:48.488715Z","iopub.status.busy":"2024-04-28T14:59:48.488284Z","iopub.status.idle":"2024-04-28T14:59:48.502525Z","shell.execute_reply":"2024-04-28T14:59:48.501643Z","shell.execute_reply.started":"2024-04-28T14:59:48.488681Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:48.504111Z","iopub.status.busy":"2024-04-28T14:59:48.503757Z","iopub.status.idle":"2024-04-28T14:59:49.014625Z","shell.execute_reply":"2024-04-28T14:59:49.013851Z","shell.execute_reply.started":"2024-04-28T14:59:48.504079Z"},"trusted":true},"outputs":[],"source":["d_obj = Discriminator(1, n_layers = 3).to(device)\n","d_ins = Discriminator(5, n_layers = 3).to(device)\n","generator = Generator(4).to(device)\n","refine_module = EdgeRefineModule().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:49.015952Z","iopub.status.busy":"2024-04-28T14:59:49.015640Z","iopub.status.idle":"2024-04-28T14:59:49.020106Z","shell.execute_reply":"2024-04-28T14:59:49.019279Z","shell.execute_reply.started":"2024-04-28T14:59:49.015929Z"},"trusted":true},"outputs":[],"source":["# !rm -rf validation_output train_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:49.024753Z","iopub.status.busy":"2024-04-28T14:59:49.024445Z","iopub.status.idle":"2024-04-28T14:59:49.995870Z","shell.execute_reply":"2024-04-28T14:59:49.994518Z","shell.execute_reply.started":"2024-04-28T14:59:49.024730Z"},"trusted":true},"outputs":[],"source":["# !mkdir validation_output train_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T14:59:49.997846Z","iopub.status.busy":"2024-04-28T14:59:49.997471Z"},"trusted":true},"outputs":[],"source":["train_and_validate(train_loader, val_loader, d_obj, d_ins, generator,refine_module, mapping)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4703440,"sourceId":7989687,"sourceType":"datasetVersion"},{"datasetId":4809198,"sourceId":8135510,"sourceType":"datasetVersion"},{"datasetId":4853033,"sourceId":8193883,"sourceType":"datasetVersion"},{"datasetId":4854114,"sourceId":8195245,"sourceType":"datasetVersion"},{"datasetId":4859528,"sourceId":8202600,"sourceType":"datasetVersion"},{"datasetId":4861822,"sourceId":8205489,"sourceType":"datasetVersion"},{"datasetId":4864446,"sourceId":8208905,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
